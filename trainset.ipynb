{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/MEGClass/megclass/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import re\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from sklearn.decomposition import PCA\n",
    "from shutil import copyfile\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture._gaussian_mixture import _estimate_gaussian_parameters\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.dataset_name = \"books\"\n",
    "args.gpu = 7\n",
    "args.pca = 128\n",
    "args.random_state = 42\n",
    "args.emb_dim = 768\n",
    "args.num_heads = 2\n",
    "args.batch_size = 64\n",
    "args.temp = 0.2\n",
    "args.lr = 1e-3\n",
    "args.epochs = 5\n",
    "args.accum_steps = 1\n",
    "args.max_sent = 150\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"/shared/data2/pk36/multidim/multigran\", args.dataset_name)\n",
    "new_data_path = os.path.join(\"/home/pk36/MEGClass/intermediate_data\", args.dataset_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# this is where we save all representations\n",
    "if not os.path.exists(new_data_path):\n",
    "    os.makedirs(new_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH = os.path.join(\"/shared/data2/pk36/multidim/multigran\")\n",
    "INTERMEDIATE_DATA_FOLDER_PATH = os.path.join(\"/home/pk36/MEGClass/intermediate_data\")\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    return tensor.clone().detach().cpu().numpy()\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def sentenceToClass(sent_repr, class_repr, weights):\n",
    "    # sent_repr: N x S x E\n",
    "    # class_repr: C x E\n",
    "    # weights: N x S # equals 0 for masked sentences\n",
    "\n",
    "    #cos-sim between (N x S) x E and (C x E) = N x S x C\n",
    "    m, n = sent_repr.shape[:2]\n",
    "    sentcos = cosine_similarity(sent_repr.reshape(m*n,-1), class_repr).reshape(m,n,-1)\n",
    "    sent_to_class = np.argmax(sentcos, axis=2) # N x S\n",
    "    sent_to_doc_class = np.sum(np.multiply(sent_to_class, weights), axis=1) # N x 1\n",
    "    return sent_to_doc_class\n",
    "\n",
    "def docToClass(doc_repr, class_repr):\n",
    "    # doc_repr: N x E\n",
    "    # class_repr: C x E\n",
    "\n",
    "    #cos-sim between N x E and C x E = N x C\n",
    "    doccos = cosine_similarity(doc_repr, class_repr)\n",
    "    doc_to_class = np.argmax(doccos, axis=1) # N x 1\n",
    "    return doc_to_class\n",
    "\n",
    "def evaluate_predictions(true_class, predicted_class, output_to_console=True, return_tuple=False, return_confusion=False):\n",
    "    confusion = confusion_matrix(true_class, predicted_class)\n",
    "    if return_confusion and output_to_console:\n",
    "        print(\"-\" * 80 + \"Evaluating\" + \"-\" * 80)\n",
    "        print(confusion)\n",
    "    \n",
    "    f1_micro = f1_score(true_class, predicted_class, average='micro')\n",
    "    f1_macro = f1_score(true_class, predicted_class, average='macro')\n",
    "    if output_to_console:\n",
    "        print(\"F1 micro: \" + str(f1_micro))\n",
    "        print(\"F1 macro: \" + str(f1_macro))\n",
    "    if return_tuple:\n",
    "        return f1_macro, f1_micro\n",
    "    else:\n",
    "        return {\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"f1_macro\": f1_macro\n",
    "        }\n",
    "    \n",
    "def getSentClassRepr(args):\n",
    "    with open(os.path.join(\"/home/pk36/XClass/data/intermediate_data\", args.dataset_name, f\"document_repr_lm-bbu-12-mixture-plm.pk\"), \"rb\") as f:\n",
    "        dictionary = pk.load(f)\n",
    "        class_repr = dictionary[\"class_representations\"]\n",
    "        sent_repr = dictionary[\"sent_representations\"]\n",
    "        if (args.dataset_name in [\"nyt-location\", \"books\"]):\n",
    "         return sent_repr, class_repr\n",
    "        else:\n",
    "            doc_repr = dictionary[\"document_representations\"]\n",
    "    return doc_repr, sent_repr, class_repr\n",
    "\n",
    "\n",
    "def getDSMapAndGold(args, sent_dict):\n",
    "    # get the ground truth labels for all documents and assign a \"ground truth\" label to each sentence based on its parent document\n",
    "    gold_labels = list(map(int, open(os.path.join(\"/shared/data2/pk36/multidim/multigran\", args.dataset_name, \"labels.txt\"), \"r\").read().splitlines()))\n",
    "    gold_sent_labels = []\n",
    "    # get all sent ids for each doc\n",
    "    doc_to_sent = []\n",
    "    sent_id = 0\n",
    "    for doc_id, doc in enumerate(sent_dict.values()):\n",
    "        sent_ids = []\n",
    "        for sent in doc:\n",
    "            sent_ids.append(sent_id)\n",
    "            gold_sent_labels.append(gold_labels[doc_id])\n",
    "            sent_id += 1\n",
    "        doc_to_sent.append(sent_ids)\n",
    "            \n",
    "    return gold_labels, gold_sent_labels, doc_to_sent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-Based Sentence Embeddings, Initial Doc Embeddings, & Class Representations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceEmb(args, sent_dict, doc_to_sent, class_words, device, classonly=False):\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    num_docs = len(doc_to_sent)\n",
    "    padded_sent_repr = np.zeros((num_docs, args.max_sent, args.emb_dim))\n",
    "    sentence_mask = np.ones((num_docs, args.max_sent))\n",
    "    doc_lengths = np.zeros(num_docs, dtype=int)\n",
    "    trimmed = 0\n",
    "\n",
    "    if not classonly:\n",
    "        for doc_id in tqdm(np.arange(num_docs)):\n",
    "            sents = sent_dict[str(doc_id)]\n",
    "            num_sent = len(sents)\n",
    "            if num_sent > args.max_sent:\n",
    "                trimmed += 1\n",
    "                sents = sents[:args.max_sent]\n",
    "                num_sent = args.max_sent\n",
    "            encoded_input = tokenizer(sents, padding=True, truncation=True, return_tensors='pt')\n",
    "            encoded_input = encoded_input.to(device)\n",
    "\n",
    "            # Compute token embeddings\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "\n",
    "            # Perform pooling\n",
    "            embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "            # Normalize embeddings\n",
    "            embeddings = tensor_to_numpy(F.normalize(embeddings, p=2, dim=1))\n",
    "\n",
    "            # save the number of sentences in each document\n",
    "            doc_lengths[doc_id] = int(num_sent)\n",
    "\n",
    "            padded_sent_repr[doc_id, :embeddings.shape[0], :] = embeddings\n",
    "            # Update mask so that padded sentences are not included in attention computation\n",
    "            sentence_mask[doc_id, :num_sent] = 0\n",
    "        print(f\"Trimmed Documents: {trimmed}\")\n",
    "\n",
    "    # construct class representations\n",
    "    class_repr = np.zeros((len(class_words), args.emb_dim))\n",
    "    intro_map = {\"20News\":\"This article is about \", \"agnews\": \"This article is about \",\n",
    "                \"yelp\": \"This is \", \"nyt-coarse\":\"This article is about \", \n",
    "                \"nyt-fine\": \"This article is about \"}\n",
    "\n",
    "    print(\"Constructing Class Representations...\")\n",
    "    for class_id in tqdm(np.arange(len(class_words))):\n",
    "        # class_sent = intro_map[args.dataset_name] + \", \".join(class_words[class_id])\n",
    "        class_sent = intro_map[args.dataset_name] + class_words[class_id][0]\n",
    "        \n",
    "        print(class_sent)\n",
    "        encoded_input = tokenizer(class_sent, truncation=True, return_tensors='pt')\n",
    "        encoded_input = encoded_input.to(device)\n",
    "         # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        # Perform pooling\n",
    "        embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        # Normalize embeddings\n",
    "        embeddings = tensor_to_numpy(F.normalize(embeddings, p=2, dim=1))\n",
    "        class_repr[class_id, :] = embeddings\n",
    "    \n",
    "    if classonly:\n",
    "        return class_repr\n",
    "    else:\n",
    "        return padded_sent_repr, class_repr, doc_lengths, sentence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertSentenceEmb(args, doc_to_sent, sent_repr):\n",
    "    num_docs = len(doc_to_sent)\n",
    "    doc_lengths = np.zeros(num_docs, dtype=int)\n",
    "    # init_doc_repr = np.zeros((num_docs, args.emb_dim))\n",
    "    padded_sent_repr = np.zeros((num_docs, args.max_sent, args.emb_dim))\n",
    "    sentence_mask = np.ones((num_docs, args.max_sent))\n",
    "    trimmed = 0\n",
    "\n",
    "\n",
    "    for doc_id in tqdm(np.arange(num_docs)):\n",
    "        start_sent = doc_to_sent[doc_id][0]\n",
    "        end_sent = doc_to_sent[doc_id][-1]\n",
    "        num_sent = end_sent - start_sent + 1\n",
    "        if num_sent > args.max_sent:\n",
    "            end_sent = start_sent + args.max_sent - 1\n",
    "            num_sent = args.max_sent\n",
    "            trimmed += 1\n",
    "        embeddings = sent_repr[start_sent:end_sent+1]\n",
    "\n",
    "        # save the number of sentences in each document\n",
    "        doc_lengths[doc_id] = int(num_sent)\n",
    "\n",
    "        # Add initial doc representation\n",
    "        # init_doc_repr[doc_id, :] = np.mean(embeddings, axis=0)\n",
    "        # Add padded sentences\n",
    "        padded_sent_repr[doc_id, :embeddings.shape[0], :] = embeddings\n",
    "        # Update mask so that padded sentences are not included in attention computation\n",
    "        sentence_mask[doc_id, :num_sent] = 0\n",
    "\n",
    "    \n",
    "    print(f\"Trimmed Documents: {trimmed}\")\n",
    "\n",
    "    return padded_sent_repr, doc_lengths, sentence_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Class Weights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetClasses(padded_sent_repr, doc_lengths, class_repr, weights=None):\n",
    "    # weights: N x 150\n",
    "    class_weights = np.zeros((padded_sent_repr.shape[0], class_repr.shape[0])) # N x C\n",
    "    sent_weights = np.zeros(padded_sent_repr.shape[:2])\n",
    "\n",
    "    for doc_id in tqdm(np.arange(padded_sent_repr.shape[0])):\n",
    "        l = doc_lengths[doc_id]\n",
    "        sent_emb = padded_sent_repr[doc_id, :l, :] # S x E\n",
    "        sentcos = cosine_similarity(sent_emb, class_repr) # S x C\n",
    "        sent_to_class = np.argmax(sentcos, axis=1) # S\n",
    "        \n",
    "        # default: equal vote weight between all sentences\n",
    "        if weights is None:\n",
    "            # w = np.ones(doc_lengths[doc_id])/doc_lengths[doc_id]\n",
    "            # top cos-sim - second cos-sim\n",
    "            toptwo = np.partition(sentcos, -2)[:, -2:] # S x 2\n",
    "            toptwo = toptwo[:, 1] - toptwo[:, 0] # S\n",
    "            w = toptwo / np.sum(toptwo)\n",
    "            sent_weights[doc_id, :l] = w\n",
    "        else:\n",
    "            w = weights[doc_id, :l]\n",
    "        \n",
    "        class_weights[doc_id, :] = np.bincount(sent_to_class, weights=w, minlength=class_repr.shape[0])\n",
    "\n",
    "    if weights is None:\n",
    "        return class_weights, sent_weights\n",
    "    else:\n",
    "        return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetClassSet(padded_sent_repr, doc_lengths, class_set, alpha=None, set_weights=None):\n",
    "    # sent_weights: N x 150 -> weigh each sentence based on its contribution to the document\n",
    "    # set_weights: C x CD -> how confident each class-indicative document is\n",
    "\n",
    "    class_weights = np.zeros((padded_sent_repr.shape[0], len(class_set))) # N x C\n",
    "    if alpha is None:\n",
    "        sent_weights = np.zeros(padded_sent_repr.shape[:2]) # N x 150\n",
    "\n",
    "    for doc_id in tqdm(np.arange(padded_sent_repr.shape[0])):\n",
    "        l = doc_lengths[doc_id]\n",
    "        sent_emb = padded_sent_repr[doc_id, :l, :] # S x E\n",
    "        sentclass_dist = np.zeros((l, len(class_set))) # S x C\n",
    "\n",
    "        for class_id in np.arange(len(class_set)):\n",
    "            sentcos = cosine_similarity(sent_emb, class_set[class_id]) # S x CD\n",
    "            if set_weights is None:\n",
    "                sentclass_sim = np.mean(sentcos, axis=1) # on average, how similar each sentence is to the class set\n",
    "            else:\n",
    "                sentclass_sim = np.average(sentcos, axis=1, weights=set_weights[class_id]) # same but weighted average based on class-indicativeness\n",
    "            \n",
    "            sentclass_dist[:, class_id] = sentclass_sim\n",
    "\n",
    "\n",
    "        sent_to_class = np.argmax(sentclass_dist, axis=1) # S\n",
    "        \n",
    "        # default: equal vote weight between all sentences\n",
    "        if alpha is None:\n",
    "            # top cos-sim - second cos-sim\n",
    "            toptwo = np.partition(sentclass_dist, -2)[:, -2:] # S x 2\n",
    "            toptwo = toptwo[:, 1] - toptwo[:, 0] # S\n",
    "            w = toptwo / np.sum(toptwo)\n",
    "            sent_weights[doc_id, :l] = w\n",
    "        else:\n",
    "            w = alpha[doc_id, :l]\n",
    "        \n",
    "        class_weights[doc_id, :] = np.bincount(sent_to_class, weights=w, minlength=len(class_set))\n",
    "\n",
    "    if alpha is None:\n",
    "        return class_weights, sent_weights\n",
    "    else:\n",
    "        return class_weights\n",
    "    \n",
    "def docToClassSet(doc_emb, class_set, set_weights=None):\n",
    "    # set_weights: C x CD -> how confident each class-indicative document is\n",
    "\n",
    "    class_dist = np.zeros((doc_emb.shape[0], len(class_set))) # D x C\n",
    "    for class_id in np.arange(len(class_set)):\n",
    "        doccos = cosine_similarity(doc_emb, class_set[class_id]) # D x CD\n",
    "        if set_weights is None:\n",
    "            cos_sim = np.mean(doccos, axis=1) # on average, how similar each document is to the class set\n",
    "        else:\n",
    "            cos_sim = np.average(doccos, axis=1, weights=set_weights[class_id]) # same but weighted average based on class-indicativeness\n",
    "        \n",
    "        class_dist[:, class_id] = cos_sim\n",
    "    \n",
    "    return np.argmax(class_dist, axis=1), class_dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get initial embeddings and class representations + gold labels ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/home/pk36/XClass/data/intermediate_data\", args.dataset_name, \"dataset.pk\"), \"rb\") as f:\n",
    "    dataset = pk.load(f)\n",
    "    sent_dict = dataset[\"sent_data\"]\n",
    "    cleaned_text = dataset[\"cleaned_text\"]\n",
    "    class_names = np.array(dataset[\"class_names\"])\n",
    "with open(os.path.join(\"/home/pk36/XClass/data/intermediate_data\", args.dataset_name, \"document_repr_lm-bbu-12-mixture-plm.pk\"), \"rb\") as f:\n",
    "    reprpickle = pk.load(f)\n",
    "    class_words = reprpickle[\"class_words\"]\n",
    "gold_labels, gold_sent_labels, doc_to_sent = getDSMapAndGold(args, sent_dict)\n",
    "num_classes = len(class_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Class Imbalance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children: (4686 docs), 0.13948919449901767\n",
      "comics_graphic: (4934 docs), 0.14687146514258498\n",
      "fantasy_paranormal: (4351 docs), 0.12951717568613444\n",
      "history_biography: (4567 docs), 0.13594689527891884\n",
      "mystery_thriller_crime: (4888 docs), 0.1455021730070846\n",
      "poetry: (1226 docs), 0.03649461213311901\n",
      "romance: (4440 docs), 0.1321664582961243\n",
      "young_adult: (4502 docs), 0.13401202595701614\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(num_classes):\n",
    "    print(f'{class_names[i]}: ({np.sum(gold_labels == i)} docs), {np.sum(gold_labels == i)/len(gold_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34037566, 0.3194362 , 0.09197737, 0.07447573, 0.05572397,\n",
       "       0.05150483, 0.03794106, 0.0160015 , 0.01256368])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_size = np.array([np.sum(gold_labels == i)/len(gold_labels) for i in np.arange(len(class_names))])\n",
    "class_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04338665, 0.04594275, 0.09606501, 0.10456231, 0.11623976,\n",
       "       0.1194095 , 0.13171414, 0.16647125, 0.17620864])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(class_size)/np.sum(np.log(class_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31997/31997 [19:43<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed Documents: 155\n",
      "Constructing Class Representations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 132.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about business\n",
      "This article is about politics\n",
      "This article is about sports\n",
      "This article is about health\n",
      "This article is about education\n",
      "This article is about estate\n",
      "This article is about arts\n",
      "This article is about science\n",
      "This article is about technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31997/31997 [01:39<00:00, 320.20it/s]\n"
     ]
    }
   ],
   "source": [
    "plm_padded_sent_repr, plm_class_repr, doc_lengths, plm_sentence_mask = sentenceEmb(args, sent_dict, doc_to_sent, class_words, device)\n",
    "\n",
    "init_plm_class_set = [np.array([plm_class_repr[i]]) for i in np.arange(len(plm_class_repr))] # C x CD x E\n",
    "plm_class_set = [np.array([plm_class_repr[i]]) for i in np.arange(len(plm_class_repr))] # C x CD x E\n",
    "\n",
    "init_plm_class_weights, init_plm_sent_weights = getTargetClassSet(plm_padded_sent_repr, doc_lengths, init_plm_class_set, alpha=None, set_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_repr, class_repr = getSentClassRepr(args)\n",
    "init_class_set = [np.array([class_repr[i]]) for i in np.arange(len(class_repr))] # C x CD x E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33594/33594 [00:00<00:00, 56697.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed Documents: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33594/33594 [01:01<00:00, 549.46it/s]\n"
     ]
    }
   ],
   "source": [
    "class_set = [np.array([class_repr[i]]) for i in np.arange(len(class_repr))] # C x CD x E\n",
    "\n",
    "padded_sent_repr, doc_lengths, sentence_mask = bertSentenceEmb(args, doc_to_sent, sent_repr)\n",
    "init_class_weights, init_sent_weights = getTargetClassSet(padded_sent_repr, doc_lengths, class_set, alpha=None, set_weights=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and Evaluate Class Weights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-Oriented:\n",
      "F1 micro: 0.5027981187116747\n",
      "F1 macro: 0.5051026475634897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5027981187116747, 'f1_macro': 0.5051026475634897}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Class-Oriented:\")\n",
    "evaluate_predictions(gold_labels, np.argmax(init_class_weights, axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGClassModel(nn.Module):\n",
    "    def __init__(self, D_in, D_hidden, head, dropout=0.0):\n",
    "        super(MEGClassModel, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=D_in, num_heads=head, dropout=dropout, batch_first=True)\n",
    "        self.layernorm = nn.LayerNorm(D_in)\n",
    "        self.embd = nn.Linear(D_in,D_hidden)\n",
    "        self.attention = nn.Linear(D_hidden,1)\n",
    "        \n",
    "    def forward(self, x_org, mask=None):\n",
    "        x, mha_w = self.mha(x_org,x_org,x_org,key_padding_mask=mask)\n",
    "        x = self.layernorm(x_org+x)\n",
    "        \n",
    "        x = self.embd(x)\n",
    "        x = torch.tanh(x) # contextualized sentences\n",
    "        a = self.attention(x)\n",
    "        if mask is not None:\n",
    "            a = a.masked_fill_((mask == 1).unsqueeze(-1), float('-inf'))\n",
    "        w = torch.softmax(a, dim=1) # alpha_k\n",
    "        o = torch.matmul(w.permute(0,2,1), x) #doc \n",
    "        return o, mha_w, w, x # contextualized doc, multi-head attention weights, alpha_k, contextualized sent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(sample_outputs, class_indices, class_embds, temp=0.2):\n",
    "    k = torch.exp(torch.nn.functional.cosine_similarity(sample_outputs[:,None], class_embds, axis=2)/temp)\n",
    "    loss = -1*(torch.log(k[np.arange(len(class_indices)),class_indices]/k.sum(1))).sum()\n",
    "    return loss/len(sample_outputs)\n",
    "\n",
    "def weighted_class_contrastive_loss(args, sample_outputs, class_weights, class_embds):\n",
    "    # k: B x C, class_weights: B x C\n",
    "    numerator = torch.exp(torch.nn.functional.cosine_similarity(sample_outputs[:,None], class_embds, axis=2)/args.temp)\n",
    "    denom = numerator.sum(dim=1).unsqueeze(-1)\n",
    "    weighted_loss = -1 * (torch.log(numerator/(denom)) * class_weights).sum() # B x C -> B\n",
    "    return weighted_loss/len(sample_outputs)\n",
    "\n",
    "def classSetContrastiveLoss(args, sample_outputs, class_weights, class_set, device, set_weights=None):\n",
    "    # sample_outputs: B x E, class_weights: B x C, class_set: C x CD x E, set_weights: C x CD\n",
    "    # flatten_class_set = np.array([class_set[i][j] for i in np.arange(len(class_set)) for j in np.arange(len(class_set[i]))])\n",
    "    # flatten_class_set = torch.cat(class_set, dim=0)\n",
    "    # denom = torch.exp(torch.nn.functional.cosine_similarity(sample_outputs[:, None], flatten_class_set, dim=2)/args.temp) # B x (C*CD)\n",
    "    # if set_weights is None:\n",
    "    #     denom = torch.sum(denom, dim=1).unsqueeze(-1) # B\n",
    "    # else:\n",
    "    #     denom = torch.sum(denom * set_weights.reshape(-1,), dim=1).unsqueeze(-1) # B\n",
    "    weighted_loss = torch.zeros((sample_outputs.size(dim=0), len(class_set))).to(device) # B x C\n",
    "    for i in np.arange(len(class_set)):\n",
    "        # similarity between contextualized docs and class-indicative docs\n",
    "        numerator = torch.exp(torch.nn.functional.cosine_similarity(sample_outputs[:, None], class_set[i], dim=2)/args.temp) # B x CD\n",
    "        weighted_loss[:, i] = numerator.mean(dim=1) # B x 1\n",
    "        # if set_weights is None:\n",
    "        #     numerator = torch.sum(numerator, dim=1) # B\n",
    "        # else:\n",
    "        #     numerator = torch.sum(torch.mul(numerator, set_weights[i]), dim=1) # B\n",
    "        # if set_weights is None:\n",
    "        \n",
    "    return -1 * (torch.log(weighted_loss/weighted_loss.sum(dim=1).unsqueeze(-1)) * class_weights).sum() / len(sample_outputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextEmb(args, sent_representations, mask, class_set, class_weights, \n",
    "                doc_lengths, new_data_path, device):\n",
    "    sent_representations = torch.from_numpy(sent_representations)\n",
    "    mask = torch.from_numpy(mask).to(torch.bool)\n",
    "    class_weights = torch.from_numpy(class_weights)\n",
    "    dataset = TensorDataset(sent_representations, mask, class_weights)\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataset_loader = DataLoader(dataset, sampler=sampler, batch_size=args.batch_size, shuffle=False)\n",
    "    # sent_representations: N docs x L sentences x 768 emb (L with padding is always max_sents=50)\n",
    "    model = MEGClassModel(args.emb_dim, args.emb_dim, args.num_heads).to(device)\n",
    "\n",
    "    total_steps = len(dataset_loader) * args.epochs / args.accum_steps\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n",
    "\n",
    "    print(\"Starting to train!\")\n",
    "\n",
    "    for i in tqdm(range(args.epochs)):\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataset_loader):\n",
    "            model.train()\n",
    "            input_emb = batch[0].to(device).float()\n",
    "            input_mask = batch[1].to(device)\n",
    "            input_weights = batch[2].to(device).float()\n",
    "            \n",
    "            c_doc, _, alpha, c_sent = model(input_emb, mask=input_mask)\n",
    "            c_doc = c_doc.squeeze(1)\n",
    "            torch_class_set = [torch.from_numpy(class_set[i]).float().to(device) for i in np.arange(len(class_set))]\n",
    "            loss = classSetContrastiveLoss(args, c_doc, input_weights, torch_class_set, device)/ args.accum_steps\n",
    "            # loss = weighted_class_contrastive_loss(args, c_doc, input_weights, torch.from_numpy(class_repr).float().to(device)) / args.accum_steps\n",
    "\n",
    "            total_train_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(dataset_loader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(new_data_path, f\"{args.dataset_name}_model_e{args.epochs}.pth\"))\n",
    "\n",
    "    print(\"Starting to evaluate!\")\n",
    "\n",
    "    evalsampler = SequentialSampler(dataset)\n",
    "    eval_loader = DataLoader(dataset, sampler=evalsampler, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    doc_predictions = None\n",
    "    attention_weights = np.zeros_like(mask, dtype=float)\n",
    "    updated_sent_repr = np.zeros_like(sent_representations)\n",
    "    final_doc_emb = np.zeros((len(class_weights), args.emb_dim))\n",
    "    idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader):\n",
    "            input_emb = batch[0].to(device).float()\n",
    "            input_mask = batch[1].to(device)\n",
    "\n",
    "            c_doc, _, alpha, c_sent = model(input_emb, mask=input_mask)\n",
    "            c_doc = c_doc.squeeze(1)\n",
    "            c_sent, c_doc, alpha = tensor_to_numpy(c_sent), tensor_to_numpy(c_doc), tensor_to_numpy(torch.squeeze(alpha, dim=2))\n",
    "\n",
    "            final_doc_emb[idx:idx+c_doc.shape[0], :] = c_doc\n",
    "            attention_weights[idx:idx+c_doc.shape[0], :] = alpha\n",
    "            updated_sent_repr[idx:idx+c_doc.shape[0], :, :] = c_sent\n",
    "\n",
    "            idx += c_doc.shape[0]\n",
    "\n",
    "            doc_class, _ = docToClassSet(c_doc, class_set)\n",
    "            if doc_predictions is None:\n",
    "                doc_predictions = doc_class\n",
    "            else:\n",
    "                doc_predictions = np.append(doc_predictions, doc_class)\n",
    "    \n",
    "    updated_class_weights = getTargetClassSet(updated_sent_repr, doc_lengths, class_set, attention_weights)\n",
    "\n",
    "    return doc_predictions, final_doc_emb, updated_sent_repr, attention_weights, updated_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/MEGClass/megclass/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:22<00:00, 23.69it/s]\n",
      " 25%|██▌       | 1/4 [00:22<01:06, 22.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.118612766265869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:18<00:00, 27.87it/s]\n",
      " 50%|█████     | 2/4 [00:41<00:40, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.8127846717834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:19<00:00, 27.25it/s]\n",
      " 75%|███████▌  | 3/4 [01:00<00:19, 19.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.3866466283798218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:18<00:00, 27.81it/s]\n",
      "100%|██████████| 4/4 [01:19<00:00, 19.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.1912039518356323\n",
      "Starting to evaluate!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:20<00:00, 25.78it/s]\n",
      "100%|██████████| 33594/33594 [01:02<00:00, 537.99it/s]\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 4\n",
    "args.lr = 1e-3\n",
    "args.temp = 0.1\n",
    "doc_to_class, final_doc_emb, updated_sent_repr, updated_sent_weights, updated_class_weights = contextEmb(args, padded_sent_repr, sentence_mask, init_class_set, init_class_weights, doc_lengths, new_data_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.5133357147109603\n",
      "F1 macro: 0.5136440314075857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5133357147109603, 'f1_macro': 0.5136440314075857}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 4\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Iteration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33594/33594 [06:20<00:00, 88.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 micro: 0.5201524081681252\n",
      "F1 macro: 0.5128400801910938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5201524081681252, 'f1_macro': 0.5128400801910938}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_class_weights = getTargetClassSet(updated_sent_repr, doc_lengths, class_set, alpha=updated_sent_weights, set_weights=None)\n",
    "evaluate_predictions(gold_labels, np.argmax(curr_class_weights, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/MEGClass/megclass/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:20<00:00, 25.46it/s]\n",
      " 25%|██▌       | 1/4 [00:20<01:01, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.145280122756958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:20<00:00, 25.59it/s]\n",
      " 50%|█████     | 2/4 [00:41<00:41, 20.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.7258243560791016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:18<00:00, 28.20it/s]\n",
      " 75%|███████▌  | 3/4 [00:59<00:19, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.0600605010986328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:18<00:00, 28.95it/s]\n",
      "100%|██████████| 4/4 [01:18<00:00, 19.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7097876667976379\n",
      "Starting to evaluate!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:23<00:00, 22.64it/s]\n",
      "100%|██████████| 33594/33594 [01:03<00:00, 529.02it/s]\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 4\n",
    "args.lr = 1e-3\n",
    "args.temp = 0.1\n",
    "doc_to_class, final_doc_emb, updated_sent_repr, updated_sent_weights, updated_class_weights = contextEmb(args, padded_sent_repr, sentence_mask, init_class_set, curr_class_weights, doc_lengths, new_data_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.48871822349229027\n",
      "F1 macro: 0.47107252805139477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.48871822349229027, 'f1_macro': 0.47107252805139477}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 4; FOURTH ITER (USING INITIAL REPRESENTATIONS)\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.5086324938977198\n",
      "F1 macro: 0.49307389487809583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5086324938977198, 'f1_macro': 0.49307389487809583}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 4; THIRD ITER (USING INITIAL REPRESENTATIONS)\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.5250937667440614\n",
      "F1 macro: 0.5161383273632094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5250937667440614, 'f1_macro': 0.5161383273632094}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 4 SECOND ITER (USING INITIAL REPRESENTATIONS)\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.7717531195792066\n",
      "F1 macro: 0.7618630503676733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': [[4713, 30, 116, 26, 6],\n",
       "  [344, 3281, 144, 198, 12],\n",
       "  [1509, 111, 1770, 536, 26],\n",
       "  [64, 70, 109, 2056, 326],\n",
       "  [44, 28, 133, 247, 1972]],\n",
       " 'f1_micro': 0.7717531195792066,\n",
       " 'f1_macro': 0.7618630503676733}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 4 SECOND ITER (USING UPDATED REPRESENTATIONS)\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.7652061999888087\n",
      "F1 macro: 0.7372566265293617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': [[4798, 56, 4, 31, 2],\n",
       "  [176, 3698, 1, 100, 4],\n",
       "  [1763, 312, 1021, 820, 36],\n",
       "  [49, 162, 3, 2253, 158],\n",
       "  [41, 44, 10, 424, 1905]],\n",
       " 'f1_micro': 0.7652061999888087,\n",
       " 'f1_macro': 0.7372566265293617}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 3\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Predictions (Document-Based): \n",
      "F1 micro: 0.7949191427452297\n",
      "F1 macro: 0.7772314826599722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': [[4830, 30, 6, 18, 7],\n",
       "  [163, 3715, 5, 91, 5],\n",
       "  [1683, 182, 1439, 614, 34],\n",
       "  [42, 121, 13, 2255, 194],\n",
       "  [47, 31, 15, 364, 1967]],\n",
       " 'f1_micro': 0.7949191427452297,\n",
       " 'f1_macro': 0.7772314826599722}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: 5\n",
    "doc_pred = np.rint(doc_to_class)\n",
    "print(\"Evaluate Predictions (Document-Based): \")\n",
    "evaluate_predictions(gold_labels, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['computer', 'sports', 'science', 'politics', 'religion'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CO Class Weights:  [0.01458973 0.00454681 0.88410401 0.04704762 0.04971184]\n",
      "Initial PLM Class Weights:  [0.12874045 0.12084435 0.44067873 0.17724041 0.13249605]\n",
      "Updated Class Weights:  [0.         0.         0.99999993 0.         0.        ]\n",
      "True Class:  2\n",
      "science ([-0.02891412  0.05887069  0.21833655 -0.00544262 -0.02173259]) 0.052025001496076584: re how to diagnose lyme... really x-newsreader tin version 1.1 pl9 gordon banks wrote in article ( marc gabriel) writes now, i'm not saying that culturing is the best way to diagnose it's very hard to culture bb in most cases.\n",
      "science ([-0.04523567  0.04875416  0.19182204 -0.02786728 -0.04778766]) 0.06097852438688278: the point is that dr. n has developed a \"feel\" for what is and what isn't ld.\n",
      "science ([-0.07668495  0.00333512  0.12947233  0.00652326 -0.12845935]) 0.029841937124729156: this comes from years of experience.\n",
      "science ([-0.06803242  0.03180561  0.14810391 -0.01367903 -0.08580509]) 0.037689365446567535: no serology can match that.\n",
      "science ([-0.09222376  0.02612415  0.12478653  0.00950981 -0.04962186]) 0.033632341772317886: unfortunately, some would call dr. n a \"quack\" and accuse him of trying to make a quick buck.\n",
      "science ([-0.09185875 -0.0133945   0.06626418  0.01116033 -0.00204415]) 0.028154153376817703: why do you think he would be called a quack?\n",
      "science ([-0.06063775 -0.01105442  0.14774548  0.08638355  0.03105862]) 0.05834987014532089: the quacks don't do cultures.\n",
      "science ([-0.04844904  0.08829701  0.21848351  0.02334673 -0.09379854]) 0.042340513318777084: they poo-poo doing more lab tests \"this is lyme, believe me, i've seen it many times.\n",
      "science ([-0.06318808  0.00982274  0.18370198 -0.06861409 -0.13936862]) 0.040676891803741455: the lab tests aren't accurate.\n",
      "science ([-0.04756132  0.04419962  0.19615238  0.00948338 -0.09390957]) 0.039576172828674316: we'll treat it now.\"\n",
      "science ([-0.01410639  0.06139212  0.21196378 -0.02348032 -0.0844851 ]) 0.03928374871611595: also, is dr. n's practice almost exclusively devoted to treating lyme patients?\n",
      "science ([ 0.03081551  0.07870621  0.20829483  0.03842439 -0.06839551]) 0.03535408526659012: i don't know any orthopedic surgeons who fit this pattern.\n",
      "science ([ 0.05674837  0.0542084   0.12899328 -0.02497261 -0.12732586]) 0.03995693847537041: they are usually gps.\n",
      "science ([-0.00958901  0.08320315  0.15927416 -0.04919305 -0.10544689]) 0.03978370875120163: no, he does not exclusively treat ld patients.\n",
      "science ([-0.04821917  0.05785093  0.17301485 -0.03405245 -0.14601549]) 0.033967528492212296: however, in some parts of the country, you don't need to be known as an ld \"specialist\" to see a large number of ld patients walk through your office.\n",
      "science ([ 0.00690285  0.16143507  0.23721075  0.03158938 -0.07910373]) 0.04032212495803833: given the huge problem of underdiagnosis, orthopedists encounter late manifestations of the disease just about every day in their regular practices.\n",
      "science ([ 5.18804232e-05  1.33945680e-01  1.75097688e-01 -2.53554471e-02\n",
      " -1.23207157e-01]) 0.031696662306785583: dr. n. told me that last year, he sent between 2 and 5 patients a week to the ld specialists... and he is not the only orthopedists in the town.\n",
      "science ([-0.117687    0.00200035  0.11843725 -0.0505308  -0.09521917]) 0.027371415868401527: let's say that only 2 people per week actually have ld.\n",
      "science ([-0.10874676  0.05365747  0.14325807 -0.03366248 -0.11153993]) 0.03987771272659302: that means at the very minimum 104 people in our town (and immediate area) develop late stage manifestations of ld every year .\n",
      "science ([-0.02245136  0.09404839  0.24898189  0.06694707 -0.06089944]) 0.035800572484731674: add in the folks who were diagnosed by neurologists, rheumatologists, gps, etc, and you can see what kind of problem we have.\n",
      "science ([-0.02637578  0.06518452  0.13642003 -0.05970395 -0.12016651]) 0.03566250205039978: no wonder just about everybody in town personally knows an ld patient.\n",
      "science ([-0.01097649  0.15886808  0.17285421 -0.02217854 -0.12498673]) 0.032453518360853195: he refers most patients to ld specialists, but in extreme cases he puts the patient on medication immediately to minimize the damage (in most cases, to the knees).\n",
      "science ([-0.03141535  0.06256541  0.1558015  -0.05286852 -0.13412712]) 0.038543667644262314: gordon is correct when he states that most ld specialists are gps.\n",
      "sports ([-0.056181    0.04462243  0.04352652  0.03214884 -0.08670989]) 0.028087258338928223: -marc.\n",
      "science ([ 0.05257455  0.04336579  0.19869965  0.02541061 -0.08976686]) 0.040097855031490326: -- -- --------------------------------------------------------------------- marc c. gabriel - u.c.\n",
      "science ([ 0.04605594  0.02732553  0.19908529  0.06019697 -0.03459684]) 0.03847586363554001: box 545 - (215) 882-0138 lehigh university\n"
     ]
    }
   ],
   "source": [
    "doc_id = 0\n",
    "print(\"Initial CO Class Weights: \", init_class_weights[doc_id])\n",
    "print(\"Initial PLM Class Weights: \", init_plm_class_weights[doc_id])\n",
    "\n",
    "print(\"Updated Class Weights: \", updated_class_weights[doc_id])\n",
    "print(\"True Class: \", gold_labels[doc_id])\n",
    "tok_sents = sent_dict[str(doc_id)]\n",
    "chosen_class = np.argmax(cosine_similarity(updated_sent_repr[doc_id, :len(tok_sents)], plm_class_repr), axis=1)\n",
    "sent_classes = cosine_similarity(updated_sent_repr[doc_id, :len(tok_sents)], plm_class_repr)\n",
    "\n",
    "for s,c,cw,w in zip(tok_sents, class_names[chosen_class.astype(int)], sent_classes, updated_sent_weights[doc_id, :len(tok_sents)]):\n",
    "    print(f'{c} ({cw}) {w}: {s}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA on Document Embeddings & Fit GMM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained document variance: 0.8804910204318771\n"
     ]
    }
   ],
   "source": [
    "_pca = PCA(n_components=64, random_state=args.random_state)\n",
    "pca_doc_repr = _pca.fit_transform(final_doc_emb)\n",
    "# pca_doc_repr = _pca.fit_transform(np.sum(padded_sent_repr, axis=1)/doc_lengths.reshape((-1, 1)))\n",
    "# pca_doc_repr = _pca.fit_transform(doc_repr)\n",
    "# pca_class_repr = [_pca.transform(class_set[i]) for i in np.arange(len(class_set))]\n",
    "pca_class_repr = [_pca.transform(init_class_set[i]) for i in np.arange(len(init_class_set))]\n",
    "# pca_class_repr = _pca.transform(plm_class_repr)\n",
    "print(f\"Explained document variance: {sum(_pca.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Document Cosine Similarity Predictions: \n",
      "F1 micro: 0.5275942132523664\n",
      "F1 macro: 0.513313956403899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.5275942132523664, 'f1_macro': 0.513313956403899}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_class_assignment, doc_class_probs = docToClassSet(pca_doc_repr, pca_class_repr, None)\n",
    "doc_class_probs = doc_class_probs[np.arange(pca_doc_repr.shape[0]), doc_class_assignment]\n",
    "\n",
    "print(\"Evaluate Document Cosine Similarity Predictions: \")\n",
    "evaluate_predictions(gold_labels, doc_class_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Document Cosine Similarity Predictions: \n",
      "F1 micro: 0.7898270941749203\n",
      "F1 macro: 0.7651374709330063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': [[4713, 47, 73, 24, 34],\n",
       "  [71, 3703, 55, 105, 45],\n",
       "  [1309, 136, 1771, 587, 149],\n",
       "  [24, 81, 34, 1678, 808],\n",
       "  [12, 17, 18, 127, 2250]],\n",
       " 'f1_micro': 0.7898270941749203,\n",
       " 'f1_macro': 0.7651374709330063}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class-oriented final doc\n",
    "# cosine_similarities = cosine_similarity(pca_doc_repr, pca_class_repr)\n",
    "# doc_class_assignment = np.argmax(cosine_similarities, axis=1)\n",
    "# doc_class_probs = cosine_similarities[np.arange(pca_doc_repr.shape[0]), doc_class_assignment]\n",
    "# doc_class_probs = updated_class_weights[np.arange(pca_doc_repr.shape[0]), doc_class_assignment]\n",
    "doc_class_assignment = docToClassSet(pca_doc_repr, pca_class_repr, None)\n",
    "\n",
    "print(\"Evaluate Document Cosine Similarity Predictions: \")\n",
    "evaluate_predictions(gold_labels, doc_class_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Document Cosine Similarity Predictions: \n",
      "F1 micro: 0.8893947368421051\n",
      "F1 macro: 0.8893947349271942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': [[16901, 2099], [2104, 16896]],\n",
       " 'f1_micro': 0.8893947368421051,\n",
       " 'f1_macro': 0.8893947349271942}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class-oriented average\n",
    "cosine_similarities = cosine_similarity(pca_doc_repr, pca_class_repr)\n",
    "doc_class_assignment = np.argmax(cosine_similarities, axis=1)\n",
    "doc_class_probs = cosine_similarities[np.arange(pca_doc_repr.shape[0]), doc_class_assignment]\n",
    "\n",
    "print(\"Evaluate Document Cosine Similarity Predictions: \")\n",
    "evaluate_predictions(gold_labels, doc_class_assignment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pseudo Training Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_dir(text, labels, prob, data_path, new_data_path):\n",
    "    assert len(text) == len(labels)\n",
    "    print(\"Saving files in:\", new_data_path)\n",
    "    \n",
    "    with open(os.path.join(new_data_path, \"dataset.txt\"), \"w\") as f:\n",
    "        for i, line in enumerate(text):\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    with open(os.path.join(new_data_path, \"labels.txt\"), \"w\") as f:\n",
    "        for i, line in enumerate(labels):\n",
    "            f.write(str(line))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    with open(os.path.join(new_data_path, \"probs.txt\"), \"w\") as f:\n",
    "        for i, line in enumerate(prob):\n",
    "            f.write(str(line))\n",
    "            #f.write(\",\".join(map(str, line)))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    copyfile(os.path.join(data_path, \"classes.txt\"),\n",
    "             os.path.join(new_data_path, \"classes.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(documents_to_class, prob, num_classes, cleaned_text, gold_labels, data_path, new_data_path, thresh=0.5):\n",
    "    pseudo_document_class_with_confidence = [[] for _ in range(num_classes)]\n",
    "    for i in range(documents_to_class.shape[0]):\n",
    "        pseudo_document_class_with_confidence[documents_to_class[i]].append((prob[i], i))\n",
    "\n",
    "    selected = []\n",
    "    confident_documents = [[] for _ in range(num_classes)]\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        pseudo_document_class_with_confidence[i] = sorted(pseudo_document_class_with_confidence[i], key=lambda x: x[0], reverse=True)\n",
    "        num_docs_to_take = int(len(pseudo_document_class_with_confidence[i]) * thresh)\n",
    "        confident_documents[i] = pseudo_document_class_with_confidence[i][:num_docs_to_take]\n",
    "        selected.extend([x[1] for x in confident_documents[i]])\n",
    "    \n",
    "    selected = sorted(selected)\n",
    "    text = [cleaned_text[i] for i in selected]\n",
    "    classes = [documents_to_class[i] for i in selected]\n",
    "    probs = [prob[i] for i in selected]\n",
    "    ###\n",
    "    gold_classes = [gold_labels[i] for i in selected]\n",
    "    evaluate_predictions(gold_classes, classes)\n",
    "    ###\n",
    "    # write_to_dir(text, classes, probs, data_path, new_data_path)\n",
    "    return confident_documents\n",
    "\n",
    "def updateClassSet(confident_docs, doc_emb, class_set):\n",
    "    for i in np.arange(len(class_set)):\n",
    "        doc_ind = [d[1] for d in confident_docs[i]]\n",
    "        class_set[i] = np.concatenate((np.expand_dims(class_set[i][0], axis=0), doc_emb[doc_ind, :]), axis=0)\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Document Cosine Similarity Predictions: \n",
      "F1 micro: 0.7743271221532091\n",
      "F1 macro: 0.7638439775669024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.7743271221532091, 'f1_macro': 0.7638439775669024}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_class_assignment, doc_class_probs = docToClassSet(final_doc_emb, init_class_set, None)\n",
    "doc_class_probs = doc_class_probs[np.arange(final_doc_emb.shape[0]), doc_class_assignment]\n",
    "\n",
    "print(\"Evaluate Document Cosine Similarity Predictions: \")\n",
    "evaluate_predictions(gold_labels, doc_class_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 micro: 0.6315350720495415\n",
      "F1 macro: 0.620464459861108\n"
     ]
    }
   ],
   "source": [
    "finalconf = generateDataset(doc_class_assignment, doc_class_probs, num_classes, cleaned_text, gold_labels, data_path, new_data_path, 0.5)\n",
    "# updateClassSet(finalconf, final_doc_emb, class_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: 4850\n",
      "politics: 4525\n",
      "sports: 1571\n",
      "health: 1182\n",
      "education: 993\n",
      "estate: 1229\n",
      "arts: 604\n",
      "science: 168\n",
      "technology: 875\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(len(finalconf)):\n",
    "    print(f'{class_names[i]}: {len(finalconf[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'politics', 'sports', 'health', 'education', 'estate',\n",
       "       'arts', 'science', 'technology'], dtype='<U10')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  children\n",
      "0.9348837209302325\n",
      "Class:  comics_graphic\n",
      "0.9668556476232011\n",
      "Class:  fantasy_paranormal\n",
      "0.43478260869565216\n",
      "Class:  history_biography\n",
      "0.9775561097256857\n",
      "Class:  mystery_thriller_crime\n",
      "0.8326235093696763\n",
      "Class:  poetry\n",
      "0.5741728922091782\n",
      "Class:  romance\n",
      "0.4310094408133624\n",
      "Class:  young_adult\n",
      "0.40095238095238095\n"
     ]
    }
   ],
   "source": [
    "# class_id = 2\n",
    "for class_id in np.arange(len(class_names)):\n",
    "    print(\"Class: \", class_names[class_id])\n",
    "    # results = [doc_class_assignment[i[1]] == gold_labels[i[1]] for i in finalconf[class_id][:50]]\n",
    "    # print(np.sum(results)/len(results))\n",
    "    all_results = [doc_class_assignment[i[1]] == gold_labels[i[1]] for i in finalconf[class_id]]\n",
    "    print(np.sum(all_results)/len(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Re SATANIC TOUNGES In article (Paul Hudson Jr) writes In article (Allen Koberg) writes Hmmm...in the old testament story about the tower of Babel, we see how God PUNISHED by giving us different language. Can we assume then that if angels have their own language at all, that they have the SAME one amongst other angels? After all, THEY were not punished in any manner. If the languages we sepak are the result of Babel, then it stands to reason that angels would speak a different language from us. You do have a valid point about multiple angelic languages. But angelic beings maybe of different species so to speak. maybe different species communicate differently. I don\\'t know either. Truth be known, so little is known of angels to even guess. All we really know is that angels ALWAYS speak in the nativ tongue of the person they\\'re talking to, so perhaps they don\\'t have ANY language of their own. Trouble is, while such stories abound, any and all attempts at verification (and we are to test the spirit...) either show that the witness had no real idea of the circumstances, or that outright fabrication was involved. The Brother Puka story in a previous post seems like a \"friend of a friend\" thing. And linguistically, a two syllable word hardly qualifies as language, inflection or no. I have heard an eyewitness account, myself. Such things are hard to prove. They don\\'t lend themselves to a laboratory thing very well. I don\\' t know if it is a very holy thing to take gifts into a laboratory anyway. Well, we are told to test the spirits. While you could do this scripturally, to see if someones claims are backed by the bible, I see nothing wrong with making sure that that guy Lazarus really was dead and now he\\'s alive. Much as many faith healers have trouble proving their \"victories\" (since most ailments \"cured\" are just plain unprovable) and modern day ressurrections have never been validated, so is it true that no modern day xenoglossolalia has been proved by clergy OR lay. That\\'s an unprovable statement. How can you prove if somethings been proved? There is no way to know that you\\'ve seen all the evidence. Once I saw an orthodontists records complete with photographs showing how one of his patients severe underbite was cured by constant prayer. It\\'s a common fallacy you commit. The non-falsifiability trick. How can I prove it when not all the evidence may be seen? Answer I can\\'t. The fallacy is in assuming that it is up to me to prove anything. When I say it has never been proven, I\\'m talking about the ones making the claims, not the skeptics, who are doing the proving. The burden of proof rest with the claimant. Unfortunately, (pontification warning) our legal system seems to be headed in the dangerous realm of making people prove their innocence (end pontification). But truthfully, Corinthians was so poorly written (or maybe just so poorly translated into English) that much remains unknown about just what Paul really intended (despite claims of hard proof one way or another). Some will see his writings in 1 cor 12-14 as saying don\\'t do this don\\'t do this and using sarcasm, metaphor, etc. while yet others take what he says literally sarcasms and metaphors notwithstanding. Me? When I read 1 Cor 14 about praying speaking in tongues regarding building oneself the church, I see him using compare contrast, saying do this because it build the church, while doing this builds onself (implying don\\'t do that). It\\'s a common usage of writing that we all employ, and it is easily seen how it COULD be interpreted this way. Why some do and some don\\'t is a mystery.',\n",
       " 4,\n",
       " 4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = finalconf[4][0][1]\n",
    "cleaned_text[doc_id], doc_class_assignment[doc_id], gold_labels[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7317021510492036, 9181)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalconf[4][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_gmm(args, document_representations, doc_class_representations, gold_labels):\n",
    "    num_classes = len(doc_class_representations)\n",
    "    cosine_similarities = cosine_similarity(document_representations, doc_class_representations)\n",
    "    doc_class_assignment = np.argmax(cosine_similarities, axis=1)\n",
    "\n",
    "    print(\"Evaluate Document Cosine Similarity Predictions: \")\n",
    "    evaluate_predictions(gold_labels, doc_class_assignment)\n",
    "\n",
    "    # initialize gmm based on these selected documents\n",
    "    document_class_assignment_matrix = np.zeros((document_representations.shape[0], num_classes))\n",
    "    for i in np.arange(len(document_representations)): # iterate through docs and sents\n",
    "        document_class_assignment_matrix[i][doc_class_assignment[i]] = 1.0\n",
    "\n",
    "    gmm = GaussianMixture(n_components=num_classes, covariance_type='tied',\n",
    "                          random_state=args.random_state,\n",
    "                          n_init=999, warm_start=True)\n",
    "\n",
    "    gmm._initialize(document_representations, document_class_assignment_matrix)\n",
    "    gmm.lower_bound_ = -np.infty\n",
    "\n",
    "    gmm.converged_ = True #HACK FOR NOT RANDOMLY INITIALIZING PARAMS DURING FIT\n",
    "    gmm.fit(document_representations)\n",
    "\n",
    "    documents_to_class = gmm.predict(document_representations)\n",
    "    confidence = gmm.predict_proba(document_representations)\n",
    "\n",
    "    print(\"Evaluate Document GMM Predictions: \")\n",
    "    evaluate_predictions(gold_labels, documents_to_class)\n",
    "\n",
    "    return documents_to_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_class_repr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Document Cosine Similarity Predictions: \n",
      "F1 micro: 0.7002218958027315\n",
      "F1 macro: 0.5808469084409915\n",
      "Evaluate Document GMM Predictions: \n",
      "F1 micro: 0.7940431915492078\n",
      "F1 macro: 0.6569828627562839\n"
     ]
    }
   ],
   "source": [
    "# 128\n",
    "doc_preds, doc_prob = doc_gmm(args, pca_doc_repr, np.array(pca_class_repr).squeeze(1), gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bc78f6dd8c9fb4bf4049d09abbebcbc9ae8bd98d8b55b5fea020bfdbf1269a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
